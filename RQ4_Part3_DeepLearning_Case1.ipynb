{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ4: To what extent can Machine Learning/NLP models identify the sensemaking aspect of feedback?\n",
    "\n",
    "## Part 3: Using DistilBERT To Identify Sensemaking\n",
    "\n",
    "In part 3 of RQ4, we are going to use a deep learning algorithm called DistilBERT to train a model to identify the sensemaking component in the feedback text. This uncased DistilBERT model was provided by the Hugging Face AI community. It was customised here to serve the needs of the sensemaking classifications in this study. Reference: [Text Classification](https://huggingface.co/docs/transformers/tasks/sequence_classification)\n",
    "\n",
    "### 1. Loading the Initial Libraries and the Dataset\n",
    "\n",
    "First, we need to load the initial set of libraries and the feedback data to be used in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the desired versions of the transformers and accelerate libraries\n",
    "# Note: If using a Kaggle notebook, restart the kernel and clear the outputs after this step\n",
    "! pip install -U git+https://github.com/huggingface/transformers.git\n",
    "! pip install -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:04:08.182833Z",
     "iopub.status.busy": "2023-06-05T10:04:08.182243Z",
     "iopub.status.idle": "2023-06-05T10:04:09.134813Z",
     "shell.execute_reply": "2023-06-05T10:04:09.133802Z",
     "shell.execute_reply.started": "2023-06-05T10:04:08.182801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the initial libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `Pandas` library to load the feedback data into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:04:09.136819Z",
     "iopub.status.busy": "2023-06-05T10:04:09.136276Z",
     "iopub.status.idle": "2023-06-05T10:04:09.187176Z",
     "shell.execute_reply": "2023-06-05T10:04:09.186303Z",
     "shell.execute_reply.started": "2023-06-05T10:04:09.136784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceScoreRem</th>\n",
       "      <th>Rubric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yuejing more in depth analysis is required and...</td>\n",
       "      <td>Sensemaking 1&amp;Impact 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Team 1 requested to re-do their workbook 3 to...</td>\n",
       "      <td>Impact 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The team submitted the workbook 23 days after ...</td>\n",
       "      <td>Sensemaking 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Risk assessment and report needs work as discu...</td>\n",
       "      <td>Sensemaking 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Good effort, Please refer to detailed feedbac...</td>\n",
       "      <td>Agency 2&amp;Agency 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    SentenceScoreRem                  Rubric\n",
       "0  Yuejing more in depth analysis is required and...  Sensemaking 1&Impact 1\n",
       "1  ﻿Team 1 requested to re-do their workbook 3 to...                Impact 1\n",
       "2  The team submitted the workbook 23 days after ...           Sensemaking 2\n",
       "3  Risk assessment and report needs work as discu...           Sensemaking 1\n",
       "4  \"Good effort, Please refer to detailed feedbac...       Agency 2&Agency 1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data with the Pandas library\n",
    "data = pd.read_csv('./LabelledFeedback/stage2.csv')\n",
    "\n",
    "# Isolating the required columns\n",
    "data = data[['SentenceScoreRem', 'Rubric']]\n",
    "\n",
    "# Checking the loaded data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning the Text and Creating the Target Variable\n",
    "\n",
    "Although Hugging Face's DistilBERT technique also has its own tokenization technique, we can help it along with some preliminary cleaning using the `NLTK` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:04:09.611871Z",
     "iopub.status.busy": "2023-06-05T10:04:09.611132Z",
     "iopub.status.idle": "2023-06-05T10:04:11.979090Z",
     "shell.execute_reply": "2023-06-05T10:04:11.977924Z",
     "shell.execute_reply.started": "2023-06-05T10:04:09.611836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Rubric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yuejing depth analysis required see link key c...</td>\n",
       "      <td>Sensemaking 1&amp;Impact 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>team requested workbook better original mark</td>\n",
       "      <td>Impact 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>team submitted workbook days submission date kv</td>\n",
       "      <td>Sensemaking 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>risk assessment report needs work discussed tu...</td>\n",
       "      <td>Sensemaking 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good effort please refer detailed feedback fil...</td>\n",
       "      <td>Agency 2&amp;Agency 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>q need use english communicate partiularly par...</td>\n",
       "      <td>Impact 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>part b complicated needed explain rate change ...</td>\n",
       "      <td>Sensemaking 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5756</th>\n",
       "      <td>q english exposition required</td>\n",
       "      <td>Impact 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>made two errors finding determinant part b fin...</td>\n",
       "      <td>Sensemaking 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5758</th>\n",
       "      <td>q point two circles intersect coordinates swit...</td>\n",
       "      <td>Sensemaking 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5759 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     yuejing depth analysis required see link key c...   \n",
       "1          team requested workbook better original mark   \n",
       "2       team submitted workbook days submission date kv   \n",
       "3     risk assessment report needs work discussed tu...   \n",
       "4     good effort please refer detailed feedback fil...   \n",
       "...                                                 ...   \n",
       "5754  q need use english communicate partiularly par...   \n",
       "5755  part b complicated needed explain rate change ...   \n",
       "5756                      q english exposition required   \n",
       "5757  made two errors finding determinant part b fin...   \n",
       "5758  q point two circles intersect coordinates swit...   \n",
       "\n",
       "                      Rubric  \n",
       "0     Sensemaking 1&Impact 1  \n",
       "1                   Impact 1  \n",
       "2              Sensemaking 2  \n",
       "3              Sensemaking 1  \n",
       "4          Agency 2&Agency 1  \n",
       "...                      ...  \n",
       "5754                Impact 1  \n",
       "5755           Sensemaking 1  \n",
       "5756                Impact 2  \n",
       "5757           Sensemaking 1  \n",
       "5758           Sensemaking 1  \n",
       "\n",
       "[5759 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a function to clean the feedback text\n",
    "def clean_text(text):\n",
    "\n",
    "    # Converting the characters of the text to lowercase form\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text.lower())\n",
    "\n",
    "    # Tokenizing the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Loading the stopwords from the NLTK corpus\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    # Removing the stop words from the tokenized text\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    # Joining the tokens\n",
    "    clean_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    # Returning the cleaned text\n",
    "    return clean_text\n",
    "\n",
    "# Applying the text cleaning function to the data\n",
    "data['text'] = data['SentenceScoreRem'].apply(clean_text)\n",
    "\n",
    "# Split the preprocessed data into features and target variable\n",
    "features = data['text']\n",
    "target = data['Rubric']\n",
    "\n",
    "# Checking the cleaned text\n",
    "data[['text', 'Rubric']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was the case in Part 1 of this research question, we will define a function to create a target variable called `label` that will contain the value 1 if the text contains the sensemaking component and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:04:14.180145Z",
     "iopub.status.busy": "2023-06-05T10:04:14.179215Z",
     "iopub.status.idle": "2023-06-05T10:04:14.200542Z",
     "shell.execute_reply": "2023-06-05T10:04:14.199604Z",
     "shell.execute_reply.started": "2023-06-05T10:04:14.180100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Rubric</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yuejing depth analysis required see link key c...</td>\n",
       "      <td>Sensemaking 1&amp;Impact 1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>team requested workbook better original mark</td>\n",
       "      <td>Impact 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>team submitted workbook days submission date kv</td>\n",
       "      <td>Sensemaking 2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>risk assessment report needs work discussed tu...</td>\n",
       "      <td>Sensemaking 1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good effort please refer detailed feedback fil...</td>\n",
       "      <td>Agency 2&amp;Agency 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>q need use english communicate partiularly par...</td>\n",
       "      <td>Impact 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>part b complicated needed explain rate change ...</td>\n",
       "      <td>Sensemaking 1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5756</th>\n",
       "      <td>q english exposition required</td>\n",
       "      <td>Impact 2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>made two errors finding determinant part b fin...</td>\n",
       "      <td>Sensemaking 1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5758</th>\n",
       "      <td>q point two circles intersect coordinates swit...</td>\n",
       "      <td>Sensemaking 1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5759 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     yuejing depth analysis required see link key c...   \n",
       "1          team requested workbook better original mark   \n",
       "2       team submitted workbook days submission date kv   \n",
       "3     risk assessment report needs work discussed tu...   \n",
       "4     good effort please refer detailed feedback fil...   \n",
       "...                                                 ...   \n",
       "5754  q need use english communicate partiularly par...   \n",
       "5755  part b complicated needed explain rate change ...   \n",
       "5756                      q english exposition required   \n",
       "5757  made two errors finding determinant part b fin...   \n",
       "5758  q point two circles intersect coordinates swit...   \n",
       "\n",
       "                      Rubric  label  \n",
       "0     Sensemaking 1&Impact 1      1  \n",
       "1                   Impact 1      0  \n",
       "2              Sensemaking 2      1  \n",
       "3              Sensemaking 1      1  \n",
       "4          Agency 2&Agency 1      0  \n",
       "...                      ...    ...  \n",
       "5754                Impact 1      0  \n",
       "5755           Sensemaking 1      1  \n",
       "5756                Impact 2      0  \n",
       "5757           Sensemaking 1      1  \n",
       "5758           Sensemaking 1      1  \n",
       "\n",
       "[5759 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a function to create the target variable\n",
    "def sensemaking(rub):\n",
    "\n",
    "    # If statement to check whether the text contains the sensemaking component\n",
    "    if 'Sensemaking' in rub:\n",
    "\n",
    "        return 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        return 0\n",
    "\n",
    "# Applying the function to the data\n",
    "data['label'] = data['Rubric'].apply(lambda x : sensemaking(x))\n",
    "\n",
    "# Checking the new column\n",
    "data[['text', 'Rubric', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparing the Data and Tokenizing it with DistilBERT\n",
    "\n",
    "Hugging Face's uncased DistilBERT has been developed in such a way that it works with the Hugging Face dataset format. Therefore, we need to convert our `Pandas` dataframe to this dataset form. Secondly, we can use their `evaluate` library to compute the metrics of the classification such as accuracy and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:04:15.937004Z",
     "iopub.status.busy": "2023-06-05T10:04:15.936631Z",
     "iopub.status.idle": "2023-06-05T10:04:26.983319Z",
     "shell.execute_reply": "2023-06-05T10:04:26.982046Z",
     "shell.execute_reply.started": "2023-06-05T10:04:15.936973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing the datasets and evaluate library\n",
    "pip install datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:04:35.406326Z",
     "iopub.status.busy": "2023-06-05T10:04:35.405922Z",
     "iopub.status.idle": "2023-06-05T10:04:35.533504Z",
     "shell.execute_reply": "2023-06-05T10:04:35.532437Z",
     "shell.execute_reply.started": "2023-06-05T10:04:35.406272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecd04e8f2a145f7b031176c67ae377e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optional Step: To upload your model to the hugging face community\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# You will be asked to enter any tokens you may have generated on Hugging face.\n",
    "# A Hugging Face Account is needed for this.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before transforming the data, let us split it into the train and test data using the `sklearn` library. As as the case in part 1, we will do a nice 80-20 split on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:04:42.691424Z",
     "iopub.status.busy": "2023-06-05T10:04:42.691040Z",
     "iopub.status.idle": "2023-06-05T10:04:42.701284Z",
     "shell.execute_reply": "2023-06-05T10:04:42.700242Z",
     "shell.execute_reply.started": "2023-06-05T10:04:42.691391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the features\n",
    "features = data['text']\n",
    "\n",
    "# Defining the target variable\n",
    "target = data['label']\n",
    "\n",
    "# Splitting the data into train and test data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create two data sets called train and test that are a combination of the features and target variables we defined in the previous step.\n",
    "\n",
    "Later, we will add these two dataframes to the Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:04:45.353216Z",
     "iopub.status.busy": "2023-06-05T10:04:45.352861Z",
     "iopub.status.idle": "2023-06-05T10:04:45.362899Z",
     "shell.execute_reply": "2023-06-05T10:04:45.361848Z",
     "shell.execute_reply.started": "2023-06-05T10:04:45.353185Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the combined train data set\n",
    "train = pd.DataFrame().assign(text=pd.DataFrame(X_train)['text'], label=pd.DataFrame(y_train)['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:04:47.379643Z",
     "iopub.status.busy": "2023-06-05T10:04:47.378662Z",
     "iopub.status.idle": "2023-06-05T10:04:47.388776Z",
     "shell.execute_reply": "2023-06-05T10:04:47.387752Z",
     "shell.execute_reply.started": "2023-06-05T10:04:47.379599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the combined test data set\n",
    "test = pd.DataFrame().assign(text=pd.DataFrame(X_test)['text'], label=pd.DataFrame(y_test)['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the `Auto Tokenizer` from the `transformers` library to load the use the DistilBERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:04:48.053845Z",
     "iopub.status.busy": "2023-06-05T10:04:48.053167Z",
     "iopub.status.idle": "2023-06-05T10:04:48.888509Z",
     "shell.execute_reply": "2023-06-05T10:04:48.887118Z",
     "shell.execute_reply.started": "2023-06-05T10:04:48.053810Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the Auto Tokenizer from the transformers library\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Loading a pre-trained DistilBERT tokenizer to preprocess the text field\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT has a maximum input length for text sequences. Therefore, we must truncate those sequences that go beyond this length while tokenizing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:04:48.891765Z",
     "iopub.status.busy": "2023-06-05T10:04:48.890721Z",
     "iopub.status.idle": "2023-06-05T10:04:48.898981Z",
     "shell.execute_reply": "2023-06-05T10:04:48.897831Z",
     "shell.execute_reply.started": "2023-06-05T10:04:48.891721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining a function to tokenize the text and truncate sequences of text that are longer than the maximum input length\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tokenizing function ready. Now, we can use the `datasets` library to pack the train and test data sets into a DistilBERT-friendly dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:04:54.206873Z",
     "iopub.status.busy": "2023-06-05T10:04:54.206181Z",
     "iopub.status.idle": "2023-06-05T10:04:54.569159Z",
     "shell.execute_reply": "2023-06-05T10:04:54.568205Z",
     "shell.execute_reply.started": "2023-06-05T10:04:54.206837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 4607\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 1152\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset library\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Converting the train and test datasets to a dataset format\n",
    "train = Dataset.from_pandas(train)\n",
    "test = Dataset.from_pandas(test)\n",
    "\n",
    "# Initialising a dataset dictionary\n",
    "ds = DatasetDict()\n",
    "\n",
    "# Adding the train and test datasets to the dataset dictionary\n",
    "ds['train'] = train\n",
    "ds['test'] = test\n",
    "\n",
    "# Checking the newly created dataset dictionary\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `map` function to tokenize the text. To process multiple components of the dataset simultaneously, we can set the `batched` flag to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:05:02.234489Z",
     "iopub.status.busy": "2023-06-05T10:05:02.233837Z",
     "iopub.status.idle": "2023-06-05T10:05:02.647198Z",
     "shell.execute_reply": "2023-06-05T10:05:02.646244Z",
     "shell.execute_reply.started": "2023-06-05T10:05:02.234451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1382e89e4d42477d8a5bad971ddc48ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d08ff735c54370ad3fbdfb04d37b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizing the text\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "tokenized_ds = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will create example batches to pad smaller elements to the longest batch length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:05:03.375694Z",
     "iopub.status.busy": "2023-06-05T10:05:03.375039Z",
     "iopub.status.idle": "2023-06-05T10:05:07.338076Z",
     "shell.execute_reply": "2023-06-05T10:05:07.337083Z",
     "shell.execute_reply.started": "2023-06-05T10:05:03.375658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dynamically padding sentences to the longest batch\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "# Reference: https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/data_collator#transformers.DataCollatorWithPadding\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setting up the Metrics and Model Optimizer\n",
    "\n",
    "As was the case with the machine learning models in part 1, the DistilBERT model is also a classification model. Therefore, its performance can be determined with the same four metrics:\n",
    "\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "\n",
    "We can load these metrics from the `evaluate` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:05:08.347309Z",
     "iopub.status.busy": "2023-06-05T10:05:08.346518Z",
     "iopub.status.idle": "2023-06-05T10:05:13.175652Z",
     "shell.execute_reply": "2023-06-05T10:05:13.174761Z",
     "shell.execute_reply.started": "2023-06-05T10:05:08.347257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1dcd22f3a54e498d643bda98e57947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5148af4c99b44edae08b554dfe93f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e04fd2efd64da280ca493c6261c430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e622870234b443feb711fe3809cf2a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing the evaluate library\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "import evaluate\n",
    "\n",
    "# Loading the classification model performance metrics from the evaluate library\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the four metrics, we can define functions to compute them in a proper manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:05:17.585247Z",
     "iopub.status.busy": "2023-06-05T10:05:17.584870Z",
     "iopub.status.idle": "2023-06-05T10:05:17.594700Z",
     "shell.execute_reply": "2023-06-05T10:05:17.593536Z",
     "shell.execute_reply.started": "2023-06-05T10:05:17.585216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Defining functions to apply the metrics to the traning process\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "# Defining a function to compute accuracy\n",
    "def compute_accuracy(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Defining a function to compute precision\n",
    "def compute_precision(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return precision.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Defining a function to compute recall\n",
    "def compute_recall(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return recall.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Defining a function to compute f1-score\n",
    "def compute_f1(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return f1.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an additional step, we can also designate our labels to print understanding results of the model. We will also feed these label translations to the model sequence classifier after building the model optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:05:28.108897Z",
     "iopub.status.busy": "2023-06-05T10:05:28.108535Z",
     "iopub.status.idle": "2023-06-05T10:05:28.114203Z",
     "shell.execute_reply": "2023-06-05T10:05:28.113043Z",
     "shell.execute_reply.started": "2023-06-05T10:05:28.108869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Designating the labels to meaningful categories\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "id2label = {0: \"No Sensemaking\", 1: \"Sensemaking\"}\n",
    "label2id = {\"No Sensemaking\": 0, \"Sensemaking\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT uses a TensorFlow model to train the data. It is necessary to make sure that the model is optimized to give the best results. We can use the `creater_optimizer` function in the transformers library to compile our optimizations for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:05:36.789414Z",
     "iopub.status.busy": "2023-06-05T10:05:36.789020Z",
     "iopub.status.idle": "2023-06-05T10:05:42.311800Z",
     "shell.execute_reply": "2023-06-05T10:05:42.310813Z",
     "shell.execute_reply.started": "2023-06-05T10:05:36.789382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the optimizer from the transformers library\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Setting the batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Setting the number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Setting the number of batches per epocj\n",
    "batches_per_epoch = len(tokenized_ds[\"train\"]) // batch_size\n",
    "\n",
    "# Setting the total steps in training\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "\n",
    "# Combining the optimizations together\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load DistilBERT with a pre-trained model automator that selects the model learning rate that gave the best results. It is here where we specify the labels to indicate `Sensemaking` and `No Sensemaking`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:05:49.714836Z",
     "iopub.status.busy": "2023-06-05T10:05:49.714250Z",
     "iopub.status.idle": "2023-06-05T10:05:54.277376Z",
     "shell.execute_reply": "2023-06-05T10:05:54.276491Z",
     "shell.execute_reply.started": "2023-06-05T10:05:49.714795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2451779e8164459a782658a941bf0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Loading the Auto Model from the transformers library\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "# Activation the auto model for sequence classification\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preparing the Data and Callbacks\n",
    "\n",
    "We are ready to make the final preparations for the model training process. Remember how we created the dataset dictionary for the train and test data and tokenized them. We need to move them into a train set and validation set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:06:06.681086Z",
     "iopub.status.busy": "2023-06-05T10:06:06.680695Z",
     "iopub.status.idle": "2023-06-05T10:06:07.076743Z",
     "shell.execute_reply": "2023-06-05T10:06:07.075760Z",
     "shell.execute_reply.started": "2023-06-05T10:06:06.681053Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "# Creating the tensorflow train set\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "# Creating the tensorflow validation set\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compile our model with the set of optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:06:15.380434Z",
     "iopub.status.busy": "2023-06-05T10:06:15.380022Z",
     "iopub.status.idle": "2023-06-05T10:06:15.400421Z",
     "shell.execute_reply": "2023-06-05T10:06:15.399410Z",
     "shell.execute_reply.started": "2023-06-05T10:06:15.380401Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Compiling the optimizer\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to prepare a set of callbacks to allow the model to return the results of each metric. We will use the `KerasMetricCallback` function to reference each of the metric computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:06:17.773682Z",
     "iopub.status.busy": "2023-06-05T10:06:17.773307Z",
     "iopub.status.idle": "2023-06-05T10:06:17.783141Z",
     "shell.execute_reply": "2023-06-05T10:06:17.782180Z",
     "shell.execute_reply.started": "2023-06-05T10:06:17.773653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the Keras Metric Callback function from the transformers library\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "# Initialising the callback for Accuracy\n",
    "metric_callback_acc = KerasMetricCallback(metric_fn=compute_accuracy, eval_dataset=tf_validation_set)\n",
    "\n",
    "# Initialising the callback for Precision\n",
    "metric_callback_pre = KerasMetricCallback(metric_fn=compute_precision, eval_dataset=tf_validation_set)\n",
    "\n",
    "# Initialising the callback for Recall\n",
    "metric_callback_re = KerasMetricCallback(metric_fn=compute_recall, eval_dataset=tf_validation_set)\n",
    "\n",
    "# Initialising the callback for F1-Score\n",
    "metric_callback_f1 = KerasMetricCallback(metric_fn=compute_f1, eval_dataset=tf_validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, Keras also offers a `PushToCallback` facility to save our customised model. This allows model reproducibility. We need to specify an output directory to save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:06:22.845640Z",
     "iopub.status.busy": "2023-06-05T10:06:22.845238Z",
     "iopub.status.idle": "2023-06-05T10:06:25.813716Z",
     "shell.execute_reply": "2023-06-05T10:06:25.812686Z",
     "shell.execute_reply.started": "2023-06-05T10:06:22.845609Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/thefishtalepundit/SensemakingDetectionModel into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Loading the Push to Callback function from the transformers library\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "# Initialising the callback to push the model to an output directory\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"SensemakingDetectionModel\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step before running the model is to save all our callbacks in a list to allow us to easily call them while fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:06:27.743363Z",
     "iopub.status.busy": "2023-06-05T10:06:27.742985Z",
     "iopub.status.idle": "2023-06-05T10:06:27.748408Z",
     "shell.execute_reply": "2023-06-05T10:06:27.746975Z",
     "shell.execute_reply.started": "2023-06-05T10:06:27.743330Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compiling the callbacks\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "callbacks = [metric_callback_acc, metric_callback_pre, metric_callback_re, metric_callback_f1, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training the Model\n",
    "\n",
    "We are finally ready to run our optimized uncased DistilBERT model with tensorflow. We can designate 3 epochs for this model run and observe the results. In case, the results are not desirable, we can always increase the number of epochs in subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:06:36.121433Z",
     "iopub.status.busy": "2023-06-05T10:06:36.120404Z",
     "iopub.status.idle": "2023-06-05T10:10:39.725605Z",
     "shell.execute_reply": "2023-06-05T10:10:39.724455Z",
     "shell.execute_reply.started": "2023-06-05T10:06:36.121393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "287/287 [==============================] - 103s 261ms/step - loss: 0.4231 - val_loss: 0.3150 - accuracy: 0.8750 - precision: 0.8872 - recall: 0.8926 - f1: 0.8899\n",
      "Epoch 2/3\n",
      "287/287 [==============================] - 51s 177ms/step - loss: 0.2335 - val_loss: 0.3034 - accuracy: 0.8854 - precision: 0.9037 - recall: 0.8926 - f1: 0.8981\n",
      "Epoch 3/3\n",
      "287/287 [==============================] - 47s 163ms/step - loss: 0.1516 - val_loss: 0.3241 - accuracy: 0.8872 - precision: 0.9065 - recall: 0.8926 - f1: 0.8995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc038976d10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model to the data\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs = 3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model has produced exceptionally good results, achieving an accuracy of 0.89 and a recall of 0.89 indicating less overfitting. Let us try to reproduce this model on a new piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:11:07.475396Z",
     "iopub.status.busy": "2023-06-05T10:11:07.474324Z",
     "iopub.status.idle": "2023-06-05T10:11:07.480578Z",
     "shell.execute_reply": "2023-06-05T10:11:07.479363Z",
     "shell.execute_reply.started": "2023-06-05T10:11:07.475346Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a new feedback text sample\n",
    "text = \"Your first three answers were correct. There were a lot of grammatical mistakes throughout your interview section. Please look up English Connect to improve your language skills. Your clarification for Mendel's theory was correct but it missed a few key details.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `Pipeline` from the transformers library to load our saved model. This pipline has a feature for sentiment analysis that can be repurposed here to detect sensemaking instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:12:04.141854Z",
     "iopub.status.busy": "2023-06-05T10:12:04.141473Z",
     "iopub.status.idle": "2023-06-05T10:12:05.395717Z",
     "shell.execute_reply": "2023-06-05T10:12:05.394652Z",
     "shell.execute_reply.started": "2023-06-05T10:12:04.141827Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at /kaggle/working/SensemakingDetectionModel were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/SensemakingDetectionModel and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'Sensemaking', 'score': 0.9869995713233948}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing pipeline from the transformers library\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Loading the pipeline model\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"SensemakingDetectionModel\")\n",
    "\n",
    "# Running the model on the text sample\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it accurately identified the presence of the sensemaking aspect in the text with a 99% certainty. Let us check whether it can accurately identify the absence of the sensemaking component with a new text sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:12:15.161021Z",
     "iopub.status.busy": "2023-06-05T10:12:15.160635Z",
     "iopub.status.idle": "2023-06-05T10:12:15.165454Z",
     "shell.execute_reply": "2023-06-05T10:12:15.164516Z",
     "shell.execute_reply.started": "2023-06-05T10:12:15.160990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a new text sample that does not contain the sensemaking element\n",
    "text = \"Well done! Just one change is required in the explaining of your teammates contribution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T10:13:08.275104Z",
     "iopub.status.busy": "2023-06-05T10:13:08.274727Z",
     "iopub.status.idle": "2023-06-05T10:13:09.266329Z",
     "shell.execute_reply": "2023-06-05T10:13:09.265221Z",
     "shell.execute_reply.started": "2023-06-05T10:13:08.275073Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at /kaggle/working/SensemakingDetectionModel were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/SensemakingDetectionModel and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'No Sensemaking', 'score': 0.9835424423217773}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the pipeline function in the transformers library\n",
    "# Reference: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Loading the pipeline model\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"SensemakingDetectionModel\")\n",
    "\n",
    "# Applying the model to the new text sample\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model accurately identified the absence of the sensemaking component with a 98% certainty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
